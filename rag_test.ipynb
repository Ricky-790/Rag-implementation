{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = '1st year books\\Guyton and Hall.pdf'\n",
    "loader = PyPDFLoader(filepath)\n",
    "\n",
    "text_docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(text_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1063"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This page intentionally left blank\n"
     ]
    }
   ],
   "source": [
    "# print(text_docs[40].page_content)\n",
    "print(text_docs[2].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(len(text_docs)):\n",
    "#     if text_docs[i].page_content == 'This page intentionally left blank':\n",
    "#         del text_docs[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Breaking into chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = text_splitter.split_documents(text_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12872"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The Cell and Its Functions 11\\nOrganization of the Cell 11\\nPhysical Structure of the Cell 12\\nComparison of the Animal Cell with  \\nPrecellular Forms of Life 17\\nFunctional Systems of the Cell 18\\nLocomotion of Cells 23\\nCHAPTER 3\\nGenetic Control of Protein Synthesis, Cell \\nFunction, and Cell Reproduction 27\\nGenes in the Cell Nucleus 27\\nThe DNA Code in the Cell Nucleus Is  \\nTransferred to an RNA Code in the Cell  \\nCytoplasmâ€”The Process of Transcription 30\\nSynthesis of Other Substances in the Cell 35'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[30].page_content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformation into embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "api_key = os.environ[\"GEMINI_API_KEY\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "\n",
    "gemini_embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\", google_api_key=api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import Chroma\n",
    "try: \n",
    "    #if vectorstore already exists\n",
    "    vectorstore = Chroma(persist_directory=\"gemini_vectorstore\", embedding_function=gemini_embeddings)\n",
    "    vectorstore.persist()\n",
    "except:\n",
    "    #if vectorstore does not exist\n",
    "    vectorstore = Chroma.from_documents(docs, gemini_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a retriever to search in your vectorstore\n",
    "retriever = vectorstore.as_retriever(search_type=\"similarity_score_threshold\", search_kwargs={\"score_threshold\": 0.6})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.generativeai as genai\n",
    "genai.configure(api_key=api_key)\n",
    "\n",
    "#Set model config and load model\n",
    "generation_config = {\n",
    "  \"temperature\": 2,\n",
    "  \"top_p\": 0.4,\n",
    "  \"top_k\": 32,\n",
    "  \"max_output_tokens\": 8192,\n",
    "}\n",
    "\n",
    "model = genai.GenerativeModel(model_name=\"gemini-1.5-pro-latest\",\n",
    "                              generation_config=generation_config,\n",
    "                              system_instruction=\"Your name is Lia.\\\\nYou are a helpful virtual assistant who specializes in the field of biology, chemistry, medicine. You are all aspiring doctors best study guide. Students will ask you questions related to biology, along with some scraped information from their text books. Your job is to answer the questions from the provided information, along with what you know. Try answering the questions to the best of your ability.\\n\\nprompt structure:''' Question: {Question of the student}\\n                                   Information: {Material provided by the user}\\n'''\\nYour answer should be created referencing to the information provided in the prompt.\",)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "convo = model.start_chat(\n",
    "  history=[\n",
    "    {\n",
    "      \"role\": \"user\",\n",
    "      \"parts\": [\n",
    "        \"Your name is Lia.\\\\nYou are a helpful virtual assistant who specializes in the field of biology, chemistry, medicine. You are all aspiring doctors best study guide. Students will ask you questions related to biology, along with some scraped information from their text books. Your job is to answer the questions from the provided information, along with what you know. Try answering the questions to the best of your ability.\\n\\nprompt structure:''' Question: {Question of the student}\\n                                   Information: {Material provided by the user}\\n'''\\nYour answer should be created referencing to the information provided in the prompt.\",\n",
    "      ],\n",
    "    },\n",
    "    {\n",
    "      \"role\": \"model\",\n",
    "      \"parts\": [\n",
    "        \"Okay, I'm ready. Ask me anything!  I'll do my best to answer your biology, chemistry, or medicine-related questions using the information you provide and my own knowledge base. I'm excited to be your virtual study guide!\\n\",\n",
    "      ],\n",
    "    },\n",
    "  ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat_with_gemini_model(question):\n",
    "    question = question\n",
    "\n",
    "    #retrieve information from the vectordb\n",
    "    result = retriever.invoke(question)\n",
    "    info = ''\n",
    "    if len(result) == 0:\n",
    "        print(\"No information found\")\n",
    "        return\n",
    "    elif len(result) > 2:\n",
    "        result = result[:2]\n",
    "    for i in range(len(result)):\n",
    "        #append the relevant information to the prompt\n",
    "        info+=result[i].page_content\n",
    "    \n",
    "    #create the prompt\n",
    "    prompt = f\"Question: {question}\\nInformation: {info}\"\n",
    "\n",
    "    response = convo.send_message(prompt)\n",
    "    print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = input(\"Enter your question: \")\n",
    "# use the model\n",
    "chat_with_gemini_model(question)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Same implementation but with ollama model\n",
    "Need to install ollama locally first\n",
    "\n",
    "<li>Create a file named custom-llama (no extensions to be given)</li>\n",
    "<li>Put the following in the file:<br>\n",
    "\n",
    "```FROM llama3.2```<br>\n",
    "```PARAMETER temperature 1```\n",
    "\n",
    "```PARAMETER num_ctx 4096```\n",
    "\n",
    "```SYSTEM {Replace with how you want the model to behave}.```\n",
    "</li>\n",
    "\n",
    "<li> Assuming you have ollama installed, run the two following commands in powershell:<br>\n",
    "\n",
    "```ollama pull llama3.2```<br>\n",
    "```ollama create custom-llama-model -f { file path to the custom-llama file}```\n",
    "</li>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### After completing the following steps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response: ChatResponse = chat(model='custom-llama-model', messages=[\n",
    "  {\n",
    "    'role': 'user',\n",
    "    'content': 'Hi! what can you do?',\n",
    "  },\n",
    "])\n",
    "print(response['message']['content'])\n",
    "# or access fields directly from the response object\n",
    "print(response.message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat_with_llama(question):\n",
    "    result = retriever.invoke(question)\n",
    "    info = ''\n",
    "    if len(result) == 0:\n",
    "        print(\"No information found\")\n",
    "        return\n",
    "    for i in range(len(result)):\n",
    "        info+=result[i].page_content\n",
    "    \n",
    "    response: ChatResponse = chat(model='custom-llama-model', messages=[\n",
    "  {\n",
    "    'role': 'user',\n",
    "    'content':  f\"Question: {question}\\nInformation: {info}\",\n",
    "  },\n",
    "])\n",
    "    \n",
    "    print(response['message']['content'])\n",
    "# or access fields directly from the response object\n",
    "    print(response.message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query=input(\"Enter your question: \")\n",
    "chat_with_llama(query)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag_chatbot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
